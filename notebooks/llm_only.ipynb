{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292d44e5-4ff8-4e81-8d76-24b1d4d8588a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate LLM only variant\n",
    "This notebook was tested on GPU using 2x24GB NVIDIA RTX A5000 GPUs. \n",
    "\n",
    "\n",
    "## Requirements\n",
    "```\n",
    "pip install transformers==4.38.2\n",
    "pip install accelerate==0.21.0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e805033d-dd73-4685-9cd2-bc5671908e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, MistralForCausalLM\n",
    "\n",
    "from utils import get_ego_schema, calc_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e74687-3fe3-4f87-9e49-92d9a405991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_ego_schema()\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b91a6-3359-4e58-8c8c-bf7bd3adf716",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d67699d-10f1-4772-8168-3a0d27e80ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▌                                                                                                                   | 99/500 [00:42<02:03,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40404040404040403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▉                                                                                      | 199/500 [01:22<01:42,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4020100502512563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████▌                                                         | 299/500 [02:03<01:25,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4414715719063545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 399/500 [02:43<00:46,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44611528822055135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 499/500 [03:25<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4589178356713427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:25<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 45.8 %\n",
      "Time Taken Per Iteration: 0.41157476568222046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "save_dict = {}\n",
    "\n",
    "correct, total = 0, 0\n",
    "st = time.time()\n",
    "for datum in tqdm(dataset):\n",
    "    only_prompts = tokenizer(datum['only_prompts'])\n",
    "    prompt_lengths = [len(x) - 1 for x in only_prompts['input_ids']]\n",
    "    encoded = tokenizer(datum['prompts'], return_tensors=\"pt\", padding=True)\n",
    "    labels = encoded['input_ids'].clone()\n",
    "    for idx, length in enumerate(prompt_lengths):\n",
    "        labels[idx, :-length] = -100\n",
    "    model_inputs = {x: y.to(device) for x,y in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**model_inputs)\n",
    "    loss = calc_loglikelihood(model_outputs.logits.detach(), labels)\n",
    "    pred = loss.argmin().item()\n",
    "    correct += datum['ans'] == pred\n",
    "    total += 1\n",
    "    if (total + 1) % 100 == 0:\n",
    "        print(f\"Accuracy: {correct / total}\")\n",
    "et = time.time()\n",
    "\n",
    "print(f\"Final Accuracy: {100 * correct / total} %\")\n",
    "print(f\"Time Taken Per Iteration: {(et - st) / 500}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d17a5-4c44-4b0a-9e6d-abe2b6fe6484",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating other model variants\n",
    "\n",
    "You will need a hugging face token and model permission (i.e. accept terms and conditions) to access some of the gated models below. This can be done easily. \n",
    "* LLAMA: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "* Gemma: https://huggingface.co/google/gemma-7b-it\n",
    "\n",
    "Follow this [link](https://huggingface.co/docs/hub/en/security-tokens) to create a user access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ebc8ff-cb81-48e3-9f16-161e02002a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove old model from memory\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0182169d-124f-467c-b26e-8843f206c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "token = \"PLEASE ADD\"  # add your hugging face token, also accept the terms & conditions for each model you use\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbe4e9b-f7e7-4dc4-a233-063aeb17062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▌                                                                                                                   | 99/500 [01:09<04:08,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1919191919191919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▉                                                                                      | 199/500 [02:20<03:18,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17587939698492464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████▌                                                         | 299/500 [03:33<02:27,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1806020066889632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 399/500 [04:44<01:16,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17543859649122806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 499/500 [05:57<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1743486973947896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [05:58<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 17.4 %\n",
      "Time Taken Per Iteration: 0.7169224119186401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "save_dict = {}\n",
    "\n",
    "correct, total = 0, 0\n",
    "st = time.time()\n",
    "for datum in tqdm(dataset):\n",
    "    only_prompts = tokenizer(datum['only_prompts'])\n",
    "    prompt_lengths = [len(x) - 1 for x in only_prompts['input_ids']]\n",
    "    encoded = tokenizer(datum['prompts'], return_tensors=\"pt\", padding=True)\n",
    "    labels = encoded['input_ids'].clone()\n",
    "    for idx, length in enumerate(prompt_lengths):\n",
    "        labels[idx, :-length] = -100\n",
    "    model_inputs = {x: y.to(device) for x,y in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**model_inputs)\n",
    "    loss = calc_loglikelihood(model_outputs.logits.detach(), labels)\n",
    "    pred = loss.argmin().item()\n",
    "    correct += datum['ans'] == pred\n",
    "    total += 1\n",
    "    if (total + 1) % 100 == 0:\n",
    "        print(f\"Accuracy: {correct / total}\")\n",
    "et = time.time()\n",
    "\n",
    "print(f\"Final Accuracy: {100 * correct / total} %\")\n",
    "print(f\"Time Taken Per Iteration: {(et - st) / 500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d189788-81de-4dc1-b417-299d3fa8675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove old model from memory\n",
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068c3aeb-b55c-4249-8080-c203b2723bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "token = \"PLEASE ADD\"  # add your hugging face token, also accept the terms & conditions for each model you use\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d16872-6c20-4f34-862c-eb6b7c6a554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▌                                                                                                                   | 99/500 [02:58<11:10,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3838383838383838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▉                                                                                      | 199/500 [06:00<08:55,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4221105527638191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████▌                                                         | 299/500 [09:04<06:17,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44481605351170567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 399/500 [12:10<03:16,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45614035087719296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 499/500 [15:16<00:01,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4589178356713427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [15:18<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 45.8 %\n",
      "Time Taken Per Iteration: 1.8373686685562134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "save_dict = {}\n",
    "\n",
    "correct, total = 0, 0\n",
    "st = time.time()\n",
    "for datum in tqdm(dataset):\n",
    "    only_prompts = tokenizer(datum['only_prompts'])\n",
    "    prompt_lengths = [len(x) - 1 for x in only_prompts['input_ids']]\n",
    "    encoded = tokenizer(datum['prompts'], return_tensors=\"pt\", padding=True)\n",
    "    labels = encoded['input_ids'].clone()\n",
    "    for idx, length in enumerate(prompt_lengths):\n",
    "        labels[idx, :-length] = -100\n",
    "    model_inputs = {x: y.to(device) for x,y in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**model_inputs)\n",
    "    loss = calc_loglikelihood(model_outputs.logits.detach(), labels)\n",
    "    pred = loss.argmin().item()\n",
    "    correct += datum['ans'] == pred\n",
    "    total += 1\n",
    "    if (total + 1) % 100 == 0:\n",
    "        print(f\"Accuracy: {correct / total}\")\n",
    "et = time.time()\n",
    "\n",
    "print(f\"Final Accuracy: {100 * correct / total} %\")\n",
    "print(f\"Time Taken Per Iteration: {(et - st) / 500}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clip] *",
   "language": "python",
   "name": "conda-env-clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
